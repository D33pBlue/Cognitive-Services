{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tensorflow 3\n",
    "Notes from 11/04/2018 lab lecture\n",
    "\n",
    "## Generate the model and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "0 Train accuracy: 0.96 Val accuracy: 0.921\n",
      "1 Train accuracy: 0.98 Val accuracy: 0.9364\n",
      "2 Train accuracy: 0.9 Val accuracy: 0.9446\n",
      "3 Train accuracy: 0.98 Val accuracy: 0.95\n",
      "4 Train accuracy: 1.0 Val accuracy: 0.9532\n",
      "5 Train accuracy: 0.96 Val accuracy: 0.9608\n",
      "6 Train accuracy: 1.0 Val accuracy: 0.9636\n",
      "7 Train accuracy: 0.94 Val accuracy: 0.9638\n",
      "8 Train accuracy: 0.94 Val accuracy: 0.9674\n",
      "9 Train accuracy: 0.96 Val accuracy: 0.9684\n",
      "10 Train accuracy: 0.96 Val accuracy: 0.9672\n",
      "11 Train accuracy: 0.98 Val accuracy: 0.9696\n",
      "12 Train accuracy: 0.98 Val accuracy: 0.9722\n",
      "13 Train accuracy: 0.98 Val accuracy: 0.9718\n",
      "14 Train accuracy: 0.96 Val accuracy: 0.9708\n",
      "15 Train accuracy: 0.92 Val accuracy: 0.974\n",
      "16 Train accuracy: 1.0 Val accuracy: 0.9736\n",
      "17 Train accuracy: 0.98 Val accuracy: 0.9742\n",
      "18 Train accuracy: 0.98 Val accuracy: 0.9748\n",
      "19 Train accuracy: 1.0 Val accuracy: 0.976\n",
      "20 Train accuracy: 0.98 Val accuracy: 0.9756\n",
      "21 Train accuracy: 1.0 Val accuracy: 0.9756\n",
      "22 Train accuracy: 0.98 Val accuracy: 0.976\n",
      "23 Train accuracy: 1.0 Val accuracy: 0.9766\n",
      "24 Train accuracy: 1.0 Val accuracy: 0.9776\n",
      "25 Train accuracy: 1.0 Val accuracy: 0.9754\n",
      "26 Train accuracy: 1.0 Val accuracy: 0.9762\n",
      "27 Train accuracy: 0.98 Val accuracy: 0.9774\n",
      "28 Train accuracy: 1.0 Val accuracy: 0.977\n",
      "29 Train accuracy: 1.0 Val accuracy: 0.9766\n",
      "30 Train accuracy: 1.0 Val accuracy: 0.9772\n",
      "31 Train accuracy: 0.98 Val accuracy: 0.9764\n",
      "32 Train accuracy: 1.0 Val accuracy: 0.978\n",
      "33 Train accuracy: 1.0 Val accuracy: 0.978\n",
      "34 Train accuracy: 1.0 Val accuracy: 0.9786\n",
      "35 Train accuracy: 0.98 Val accuracy: 0.9786\n",
      "36 Train accuracy: 0.98 Val accuracy: 0.9776\n",
      "37 Train accuracy: 1.0 Val accuracy: 0.9782\n",
      "38 Train accuracy: 0.98 Val accuracy: 0.9782\n",
      "39 Train accuracy: 1.0 Val accuracy: 0.9786\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# load MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\") # download files\n",
    "# define layers dimensions\n",
    "n_inputs = 28*28  # MNIST image sizen_hidden1 = 300\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "# define function to construct a layer with n_neurons ad input X; name of layer and activation\n",
    "# type for the units are additional parameters\n",
    "def neuron_layer(X, n_neurons, name, activation=None): \n",
    "    with tf.name_scope(name): # define a name scope for better visualization in TensorBoard \n",
    "        n_inputs = int(X.get_shape()[1]) \n",
    "        stddev = 2 / np.sqrt(n_inputs) # compute standard deviation for weights initialization\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev) # normal distribution\n",
    "        W = tf.Variable(init, name=\"kernel\") # create weights matrix with initializer\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\") # create biases (= 0) for each neuron\n",
    "        Z = tf.matmul(X, W) + b # create subgraph for computing the input net (weighted sum)\n",
    "        if activation is not None:\n",
    "            return activation(Z) # apply activation function if defined\n",
    "        else:\n",
    "            return Z\n",
    "\n",
    "# build the network (with no soft-max, that will be inserted in the definition of loss)\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "# build the loss: single label multi-class classification -> softmax in output + cross entropy loss \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "# build the subgraph for computing the gradient \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "# define how to evaluate the model\n",
    "with tf.name_scope(\"eval\"):\n",
    "    # check if the highest logit corresponds to the target class\n",
    "    correct = tf.nn.in_top_k(logits, y, 1) # returns a 1D tensor\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) # average to compute accuracy \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "# open session\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,y: mnist.validation.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload the model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "Predicted classes: [3 6 9 3 1 4 1 7 6 9 6 0 5 4 9 9 2 1 9 4 8 7 3 9 7 4 4 4 9 2]\n",
      "Actual classes:    [3 6 9 3 1 4 1 7 6 9 6 0 5 4 9 9 2 1 9 4 8 7 3 9 7 4 4 4 9 2]\n"
     ]
    }
   ],
   "source": [
    "# redefine name Saver()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# open session to load the trained model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\") # restore the saved model\n",
    "    X_new_scaled = mnist.test.images[90:120] # get test images from 140 to 159\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled}) # evaluate the outputs (logits)\n",
    "    y_pred = np.argmax(Z, axis=1) # select the index that reaches the maximum value\n",
    "\n",
    "print(\"Predicted classes:\", y_pred) # print the predicted index class\n",
    "print(\"Actual classes:   \", mnist.test.labels[90:120]) # print the actual index class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights intialization\n",
    "Gradient will be almost zero if initialization is wrong.\n",
    "There are many different function that can be use, and each of one can give help in learning specific tasks (can give less problem in gradient descent).\n",
    "Trade-off: for each unit normalize the initial weights with respect of the number of incoming and outcoming connections.\n",
    "Small modules for the weights are better in order to compute gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example a ReLU function initialization is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "0 Train accuracy: 0.94 Val accuracy: 0.9084\n",
      "1 Train accuracy: 0.94 Val accuracy: 0.9284\n",
      "2 Train accuracy: 0.9 Val accuracy: 0.939\n",
      "3 Train accuracy: 0.92 Val accuracy: 0.9448\n",
      "4 Train accuracy: 0.94 Val accuracy: 0.9454\n",
      "5 Train accuracy: 1.0 Val accuracy: 0.9518\n",
      "6 Train accuracy: 0.98 Val accuracy: 0.9538\n",
      "7 Train accuracy: 0.92 Val accuracy: 0.957\n",
      "8 Train accuracy: 0.94 Val accuracy: 0.9592\n",
      "9 Train accuracy: 0.94 Val accuracy: 0.9624\n",
      "10 Train accuracy: 0.94 Val accuracy: 0.9642\n",
      "11 Train accuracy: 0.96 Val accuracy: 0.966\n",
      "12 Train accuracy: 1.0 Val accuracy: 0.967\n",
      "13 Train accuracy: 1.0 Val accuracy: 0.9678\n",
      "14 Train accuracy: 0.94 Val accuracy: 0.968\n",
      "15 Train accuracy: 0.96 Val accuracy: 0.9702\n",
      "16 Train accuracy: 1.0 Val accuracy: 0.971\n",
      "17 Train accuracy: 1.0 Val accuracy: 0.97\n",
      "18 Train accuracy: 1.0 Val accuracy: 0.9716\n",
      "19 Train accuracy: 0.98 Val accuracy: 0.9728\n",
      "20 Train accuracy: 1.0 Val accuracy: 0.9726\n",
      "21 Train accuracy: 0.98 Val accuracy: 0.974\n",
      "22 Train accuracy: 1.0 Val accuracy: 0.9746\n",
      "23 Train accuracy: 1.0 Val accuracy: 0.9752\n",
      "24 Train accuracy: 1.0 Val accuracy: 0.9746\n",
      "25 Train accuracy: 0.98 Val accuracy: 0.9732\n",
      "26 Train accuracy: 0.94 Val accuracy: 0.975\n",
      "27 Train accuracy: 1.0 Val accuracy: 0.9774\n",
      "28 Train accuracy: 1.0 Val accuracy: 0.9762\n",
      "29 Train accuracy: 0.98 Val accuracy: 0.9758\n",
      "30 Train accuracy: 1.0 Val accuracy: 0.9758\n",
      "31 Train accuracy: 1.0 Val accuracy: 0.9772\n",
      "32 Train accuracy: 0.96 Val accuracy: 0.9774\n",
      "33 Train accuracy: 0.98 Val accuracy: 0.9792\n",
      "34 Train accuracy: 1.0 Val accuracy: 0.9776\n",
      "35 Train accuracy: 1.0 Val accuracy: 0.9798\n",
      "36 Train accuracy: 0.98 Val accuracy: 0.979\n",
      "37 Train accuracy: 0.98 Val accuracy: 0.9776\n",
      "38 Train accuracy: 1.0 Val accuracy: 0.9792\n",
      "39 Train accuracy: 1.0 Val accuracy: 0.9784\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# load MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\") # download files\n",
    "# define layers dimensions\n",
    "n_inputs = 28*28  # MNIST image sizen_hidden1 = 300\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "# He inizialization\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "# build the network (with no soft-max, that will be inserted in the definition of loss)                                                       \n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_initializer=he_init, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, kernel_initializer=he_init, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "# build the loss: single label multi-class classification -> softmax in output + cross entropy loss \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "# build the subgraph for computing the gradient \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "# define how to evaluate the model\n",
    "with tf.name_scope(\"eval\"):\n",
    "    # check if the highest logit corresponds to the target class\n",
    "    correct = tf.nn.in_top_k(logits, y, 1) # returns a 1D tensor\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) # average to compute accuracy \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "# open session\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,y: mnist.validation.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leaky ReLU** activation function instead of taking 0 has negative values; instead **ELU** uses non linear functions in its parts. ELU is predefined in tensorflow.\n",
    "**SELU** has another different non linear function respect to ELU and try to find the optimal shape in order to improve gradient descent. Normalization with this function give 0 mean and 1 stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "0 Train accuracy: 0.96 Val accuracy: 0.9244\n",
      "1 Train accuracy: 0.92 Val accuracy: 0.9362\n",
      "2 Train accuracy: 0.98 Val accuracy: 0.9452\n",
      "3 Train accuracy: 0.98 Val accuracy: 0.9506\n",
      "4 Train accuracy: 0.98 Val accuracy: 0.9552\n",
      "5 Train accuracy: 1.0 Val accuracy: 0.9568\n",
      "6 Train accuracy: 0.98 Val accuracy: 0.9586\n",
      "7 Train accuracy: 1.0 Val accuracy: 0.9594\n",
      "8 Train accuracy: 0.98 Val accuracy: 0.9624\n",
      "9 Train accuracy: 0.98 Val accuracy: 0.963\n",
      "10 Train accuracy: 0.98 Val accuracy: 0.9636\n",
      "11 Train accuracy: 0.98 Val accuracy: 0.9654\n",
      "12 Train accuracy: 1.0 Val accuracy: 0.9652\n",
      "13 Train accuracy: 1.0 Val accuracy: 0.9662\n",
      "14 Train accuracy: 0.96 Val accuracy: 0.9678\n",
      "15 Train accuracy: 1.0 Val accuracy: 0.9668\n",
      "16 Train accuracy: 1.0 Val accuracy: 0.9682\n",
      "17 Train accuracy: 1.0 Val accuracy: 0.9678\n",
      "18 Train accuracy: 1.0 Val accuracy: 0.9676\n",
      "19 Train accuracy: 1.0 Val accuracy: 0.9688\n",
      "20 Train accuracy: 1.0 Val accuracy: 0.9686\n",
      "21 Train accuracy: 0.98 Val accuracy: 0.969\n",
      "22 Train accuracy: 1.0 Val accuracy: 0.9678\n",
      "23 Train accuracy: 1.0 Val accuracy: 0.969\n",
      "24 Train accuracy: 1.0 Val accuracy: 0.9702\n",
      "25 Train accuracy: 1.0 Val accuracy: 0.97\n",
      "26 Train accuracy: 1.0 Val accuracy: 0.97\n",
      "27 Train accuracy: 1.0 Val accuracy: 0.9696\n",
      "28 Train accuracy: 1.0 Val accuracy: 0.9702\n",
      "29 Train accuracy: 1.0 Val accuracy: 0.9706\n",
      "30 Train accuracy: 1.0 Val accuracy: 0.9696\n",
      "31 Train accuracy: 1.0 Val accuracy: 0.9686\n",
      "32 Train accuracy: 1.0 Val accuracy: 0.97\n",
      "33 Train accuracy: 1.0 Val accuracy: 0.9686\n",
      "34 Train accuracy: 1.0 Val accuracy: 0.9702\n",
      "35 Train accuracy: 1.0 Val accuracy: 0.9698\n",
      "36 Train accuracy: 1.0 Val accuracy: 0.9698\n",
      "37 Train accuracy: 1.0 Val accuracy: 0.9692\n",
      "38 Train accuracy: 1.0 Val accuracy: 0.9704\n",
      "39 Train accuracy: 1.0 Val accuracy: 0.9702\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# load MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\") # download files\n",
    "# define layers dimensions\n",
    "n_inputs = 28*28  # MNIST image sizen_hidden1 = 300\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "# define Leaky ReLU\n",
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "# define SELU\n",
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))\n",
    "\n",
    "# build the network (with no soft-max, that will be inserted in the definition of loss)\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    # Leaky ReLU\n",
    "    # hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=leaky_relu)\n",
    "    # ELU\n",
    "    # hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.elu)\n",
    "    # SELU\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=selu)        \n",
    "    # Leaky ReLU \n",
    "    # hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=leaky_relu)\n",
    "    # ELU \n",
    "    # hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.elu)\n",
    "    # SELU\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=selu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "\n",
    "# build the loss: single label multi-class classification -> softmax in output + cross entropy loss \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "# build the subgraph for computing the gradient \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "# define how to evaluate the model\n",
    "with tf.name_scope(\"eval\"):\n",
    "    # check if the highest logit corresponds to the target class\n",
    "    correct = tf.nn.in_top_k(logits, y, 1) # returns a 1D tensor\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) # average to compute accuracy \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "# begin THIS IS NEEDED FOR SELU\n",
    "means = mnist.train.images.mean(axis=0, keepdims=True)\n",
    "stds = mnist.train.images.std(axis=0, keepdims=True) + 1e-10\n",
    "# end SELU\n",
    "\n",
    "# open session\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            # begin THIS IS NEEDED FOR SELU \n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            # end SELU\n",
    "            # sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        # begin THIS IS NEEDED FOR SELU\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        X_val_scaled = (mnist.validation.images - means) / stds\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_val_scaled, y: mnist.validation.labels})\n",
    "        # end SELU \n",
    "        # acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        # acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,y: mnist.validation.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization\n",
    "Normalizing inputs speed up training. When you've got a multiple layers at the end the output is not normalized anymore. The idea is to normalize it again. Backpropagation takes into account the normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "0 Train accuracy: 0.9 Val accuracy: 0.9264\n",
      "1 Train accuracy: 0.94 Val accuracy: 0.944\n",
      "2 Train accuracy: 0.96 Val accuracy: 0.956\n",
      "3 Train accuracy: 0.96 Val accuracy: 0.9612\n",
      "4 Train accuracy: 0.98 Val accuracy: 0.9626\n",
      "5 Train accuracy: 0.98 Val accuracy: 0.9658\n",
      "6 Train accuracy: 0.98 Val accuracy: 0.97\n",
      "7 Train accuracy: 0.96 Val accuracy: 0.9714\n",
      "8 Train accuracy: 1.0 Val accuracy: 0.9718\n",
      "9 Train accuracy: 1.0 Val accuracy: 0.9738\n",
      "10 Train accuracy: 0.98 Val accuracy: 0.9724\n",
      "11 Train accuracy: 1.0 Val accuracy: 0.9738\n",
      "12 Train accuracy: 1.0 Val accuracy: 0.9752\n",
      "13 Train accuracy: 1.0 Val accuracy: 0.977\n",
      "14 Train accuracy: 1.0 Val accuracy: 0.976\n",
      "15 Train accuracy: 1.0 Val accuracy: 0.9774\n",
      "16 Train accuracy: 1.0 Val accuracy: 0.9806\n",
      "17 Train accuracy: 1.0 Val accuracy: 0.9776\n",
      "18 Train accuracy: 1.0 Val accuracy: 0.9784\n",
      "19 Train accuracy: 1.0 Val accuracy: 0.9788\n",
      "20 Train accuracy: 1.0 Val accuracy: 0.9778\n",
      "21 Train accuracy: 1.0 Val accuracy: 0.9784\n",
      "22 Train accuracy: 1.0 Val accuracy: 0.978\n",
      "23 Train accuracy: 1.0 Val accuracy: 0.9796\n",
      "24 Train accuracy: 1.0 Val accuracy: 0.9804\n",
      "25 Train accuracy: 1.0 Val accuracy: 0.9792\n",
      "26 Train accuracy: 1.0 Val accuracy: 0.98\n",
      "27 Train accuracy: 1.0 Val accuracy: 0.9798\n",
      "28 Train accuracy: 1.0 Val accuracy: 0.9778\n",
      "29 Train accuracy: 1.0 Val accuracy: 0.9802\n",
      "30 Train accuracy: 1.0 Val accuracy: 0.9798\n",
      "31 Train accuracy: 1.0 Val accuracy: 0.9796\n",
      "32 Train accuracy: 1.0 Val accuracy: 0.979\n",
      "33 Train accuracy: 1.0 Val accuracy: 0.9788\n",
      "34 Train accuracy: 1.0 Val accuracy: 0.981\n",
      "35 Train accuracy: 1.0 Val accuracy: 0.9792\n",
      "36 Train accuracy: 1.0 Val accuracy: 0.9804\n",
      "37 Train accuracy: 1.0 Val accuracy: 0.981\n",
      "38 Train accuracy: 1.0 Val accuracy: 0.9788\n",
      "39 Train accuracy: 1.0 Val accuracy: 0.9806\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# load MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\") # download files\n",
    "# define layers dimensions\n",
    "n_inputs = 28*28  # MNIST image sizen_hidden1 = 300\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "    hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "    logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "    logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=0.9)\n",
    "\n",
    "# build the loss: single label multi-class classification -> softmax in output + cross entropy loss \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "# build the subgraph for computing the gradient \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "# define how to evaluate the model\n",
    "with tf.name_scope(\"eval\"):\n",
    "    # check if the highest logit corresponds to the target class\n",
    "    correct = tf.nn.in_top_k(logits, y, 1) # returns a 1D tensor\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) # average to compute accuracy \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "# the update operations needed by batch normalization (for computing moving averages)\n",
    "# are added to the UPDATE_OPS collection and need to be evaluated during training\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "# open session\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,y: mnist.validation.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "# print(\"\\ntf.trainable_variables()\")\n",
    "# for v in tf.trainable_variables():\n",
    "#    print(v.name)\n",
    "\n",
    "# print(\"\\ntf.global_variables()\")\n",
    "# for v in tf.global_variables():\n",
    "#    print(v.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
