{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow 4\n",
    "\n",
    "Notes from 12/04/2018 lecture\n",
    "\n",
    "## Regularization with early stopping\n",
    "\n",
    "- Evaluate the model on a validation set at regular interval of epochs\n",
    "- Store the current best performin model (on validation)\n",
    "- Stop if the best stored model is not changed for many epochs\n",
    "- Use the saved model\n",
    "\n",
    "![image](resources/im1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regualarization with extended loss\n",
    "\n",
    "Uses L1 and L2 regularization\n",
    "![immagine](resources/im2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Summary name ACC Test is illegal; using ACC_Test instead.\n",
      "0 Train accuracy: 0.96 Val accuracy: 0.887\n",
      "1 Train accuracy: 0.98 Val accuracy: 0.893\n",
      "2 Train accuracy: 0.92 Val accuracy: 0.8964\n",
      "3 Train accuracy: 0.86 Val accuracy: 0.895\n",
      "4 Train accuracy: 0.9 Val accuracy: 0.9008\n",
      "5 Train accuracy: 0.88 Val accuracy: 0.8868\n",
      "6 Train accuracy: 0.92 Val accuracy: 0.9078\n",
      "7 Train accuracy: 0.92 Val accuracy: 0.8982\n",
      "8 Train accuracy: 0.92 Val accuracy: 0.8936\n",
      "9 Train accuracy: 0.92 Val accuracy: 0.8894\n",
      "10 Train accuracy: 0.96 Val accuracy: 0.9022\n",
      "11 Train accuracy: 0.92 Val accuracy: 0.8946\n",
      "12 Train accuracy: 0.88 Val accuracy: 0.8968\n",
      "13 Train accuracy: 0.86 Val accuracy: 0.881\n",
      "14 Train accuracy: 0.94 Val accuracy: 0.8948\n",
      "15 Train accuracy: 0.94 Val accuracy: 0.8978\n",
      "16 Train accuracy: 0.9 Val accuracy: 0.9014\n",
      "17 Train accuracy: 0.92 Val accuracy: 0.8924\n",
      "18 Train accuracy: 0.96 Val accuracy: 0.8984\n",
      "19 Train accuracy: 0.92 Val accuracy: 0.8956\n",
      "20 Train accuracy: 0.9 Val accuracy: 0.8932\n",
      "21 Train accuracy: 0.92 Val accuracy: 0.9008\n",
      "22 Train accuracy: 0.88 Val accuracy: 0.893\n",
      "23 Train accuracy: 0.88 Val accuracy: 0.894\n",
      "24 Train accuracy: 0.94 Val accuracy: 0.901\n",
      "25 Train accuracy: 0.92 Val accuracy: 0.8958\n",
      "26 Train accuracy: 0.88 Val accuracy: 0.8928\n",
      "27 Train accuracy: 0.96 Val accuracy: 0.8976\n",
      "28 Train accuracy: 0.92 Val accuracy: 0.9038\n",
      "29 Train accuracy: 0.9 Val accuracy: 0.8708\n",
      "30 Train accuracy: 0.9 Val accuracy: 0.8944\n",
      "31 Train accuracy: 0.88 Val accuracy: 0.9006\n",
      "32 Train accuracy: 0.94 Val accuracy: 0.8954\n",
      "33 Train accuracy: 0.96 Val accuracy: 0.8958\n",
      "34 Train accuracy: 0.94 Val accuracy: 0.8966\n",
      "35 Train accuracy: 0.92 Val accuracy: 0.9002\n",
      "36 Train accuracy: 0.9 Val accuracy: 0.9054\n",
      "37 Train accuracy: 0.94 Val accuracy: 0.9072\n",
      "38 Train accuracy: 0.94 Val accuracy: 0.8956\n",
      "39 Train accuracy: 0.88 Val accuracy: 0.8958\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "logdir2 = \"{}/run2-{}/\".format(root_logdir, now)\n",
    "\n",
    "# load MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\") # download files\n",
    "# define layers dimensions\n",
    "n_inputs = 28*28  # MNIST image sizen_hidden1 = 300\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "# regularization hyperparameter: Loss + lambda_parameter * regularization  \n",
    "lambda_parameter = 0.003\n",
    "\n",
    "# define dense layer with L1 regularization\n",
    "# the PARTIAL() is used for partial function application which \"freezes\" \n",
    "# some portion of a function's arguments and/or keywords resulting in a \n",
    "# new object with a simplified signature\n",
    "\n",
    "# L1\n",
    "my_dense_layer = partial(tf.layers.dense, activation=tf.nn.relu, kernel_regularizer=tf.contrib.layers.l1_regularizer(lambda_parameter))\n",
    "\n",
    "# L2\n",
    "# my_dense_layer = partial(tf.layers.dense, activation=tf.nn.relu, kernel_regularizer=tf.contrib.layers.l2_regularizer(lambda_parameter))\n",
    "\n",
    "# build the network (with no soft-max, that will be inserted in the definition of loss)                                                        \n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None, name=\"outputs\")\n",
    "\n",
    "# build the loss: single label multi-class classification -> softmax in output + cross entropy loss \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "    # add the regularization losses to the base loss:\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\") \n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")\n",
    "\n",
    "# build the subgraph for computing the gradient \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov=True)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "# define how to evaluate the model\n",
    "with tf.name_scope(\"eval\"):\n",
    "    # check if the highest logit corresponds to the target class\n",
    "    correct = tf.nn.in_top_k(logits, y, 1) # returns a 1D tensor\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) # average to compute accuracy \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('ACC Test', accuracy)\n",
    "file_writer_1 = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "file_writer_2 = tf.summary.FileWriter(logdir2, tf.get_default_graph())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "# open session\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,y: mnist.validation.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val)\n",
    "        summary_str = acc_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        file_writer_1.add_summary(summary_str,epoch)\n",
    "        file_writer_1.flush()\n",
    "        summary_str = acc_summary.eval(feed_dict={X: mnist.validation.images,y: mnist.validation.labels})\n",
    "        file_writer_2.add_summary(summary_str,epoch)\n",
    "        file_writer_2.flush()\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "file_writer_1.close()\n",
    "file_writer_2.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization with Dropout\n",
    "\n",
    "Do training to choose random units to remove stochastically. Each unit is set to 0 with probability 0.5. In this way they can not co-adapt to other units. Change the way the system viewes the input and let it adapts to a input that can change. Any time the network change.\n",
    "![immagine](resources/im3.png)\n",
    "A mask is used every time. At test time however no mask should be used in order to make predictions.\n",
    "Can be viewed as a geometric average of exponential number of networks.\n",
    "Dropout is already implemented in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-\".format(root_logdir)+sys.argv[1]+\"-{}/\".format(now)\n",
    "logdir2 = \"{}/run2-\".format(root_logdir)+sys.argv[1]+\"-{}/\".format(now)\n",
    "\n",
    "# load MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\") # download files\n",
    "# define layers dimensions\n",
    "n_inputs = 28*28  # MNIST image sizen_hidden1 = 300\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = float(0.5)#sys.argv[1])  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")\n",
    "\n",
    "# build the loss: single label multi-class classification -> softmax in output + cross entropy loss \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "# build the subgraph for computing the gradient \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "# define how to evaluate the model\n",
    "with tf.name_scope(\"eval\"):\n",
    "    # check if the highest logit corresponds to the target class\n",
    "    correct = tf.nn.in_top_k(logits, y, 1) # returns a 1D tensor\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) # average to compute accuracy \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "acc_summary = tf.summary.scalar('ACC Test', accuracy)\n",
    "file_writer_1 = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "file_writer_2 = tf.summary.FileWriter(logdir2, tf.get_default_graph())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "# open session\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val)\n",
    "        summary_str = acc_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        file_writer_1.add_summary(summary_str,epoch)\n",
    "        file_writer_1.flush()\n",
    "        summary_str = acc_summary.eval(feed_dict={X: mnist.validation.images,y: mnist.validation.labels})\n",
    "        file_writer_2.add_summary(summary_str,epoch)\n",
    "        file_writer_2.flush()\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural network\n",
    "\n",
    "In this example we consider images in MNIST as sequence of rows\n",
    "![immagine](resources/im4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "0 Train accuracy: 0.94 Test accuracy: 0.9172\n",
      "1 Train accuracy: 0.94 Test accuracy: 0.948\n",
      "2 Train accuracy: 0.94666666 Test accuracy: 0.9488\n",
      "3 Train accuracy: 0.98 Test accuracy: 0.9621\n",
      "4 Train accuracy: 0.98 Test accuracy: 0.9606\n",
      "5 Train accuracy: 0.9533333 Test accuracy: 0.9646\n",
      "6 Train accuracy: 0.98 Test accuracy: 0.9715\n",
      "7 Train accuracy: 0.9866667 Test accuracy: 0.9686\n",
      "8 Train accuracy: 0.96666664 Test accuracy: 0.9683\n",
      "9 Train accuracy: 0.98 Test accuracy: 0.9768\n",
      "10 Train accuracy: 0.9866667 Test accuracy: 0.9736\n",
      "11 Train accuracy: 0.98 Test accuracy: 0.9672\n",
      "12 Train accuracy: 0.98 Test accuracy: 0.9775\n",
      "13 Train accuracy: 0.94666666 Test accuracy: 0.9756\n",
      "14 Train accuracy: 0.97333336 Test accuracy: 0.9761\n",
      "15 Train accuracy: 0.99333334 Test accuracy: 0.975\n",
      "16 Train accuracy: 0.9866667 Test accuracy: 0.9715\n",
      "17 Train accuracy: 0.98 Test accuracy: 0.9776\n",
      "18 Train accuracy: 0.98 Test accuracy: 0.9671\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b1ef3e5e6435>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mX_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0macc_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0macc_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ioson\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ioson\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ioson\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ioson\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ioson\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# load MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\") # download files\n",
    "\n",
    "# define dimensions\n",
    "n_steps = 28\n",
    "n_inputs = 28\n",
    "n_neurons = 150\n",
    "n_outputs = 10\n",
    "\n",
    "X_test = mnist.test.images.reshape((-1, n_steps, n_inputs))\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch = X_batch.reshape((-1, n_steps, n_inputs))\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
